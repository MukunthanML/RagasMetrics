### üìå **Context Precision (with and without reference) ‚Äì Explained Simply**

In **LLM evaluation**, especially in **RAG (Retrieval-Augmented Generation)** systems, **Context Precision** is a metric used to measure **how relevant the retrieved context is to the given question or query**.

---

### üß† **Definition:**

> **Context Precision** = (Number of relevant context chunks retrieved) / (Total number of context chunks retrieved)

It's used to evaluate how well your **retriever** is performing‚Äînot the generator.

---

### ‚úÖ **1. Context Precision WITH Reference**

This is **evaluated using a known ground truth reference answer**.

#### üìä How it's calculated:

* You have a **reference answer** (ideal or ground truth answer).
* You compare **each retrieved context chunk** to see if it **supports or overlaps with** the reference answer.
* **Precision** = relevant chunks / total retrieved chunks

üîç **Use Case**: When you‚Äôre doing **benchmarking or testing**, and you know the correct answer.

#### üß™ Example:

* **Question**: "What is the capital of France?"
* **Reference Answer**: "Paris"
* **Retrieved Chunks**:

  1. "Paris is the capital of France" ‚úÖ (relevant)
  2. "France is in Europe" ‚ùå (not relevant)
  3. "Eiffel Tower is in Paris" ‚úÖ (relevant)

**Precision** = 2 relevant chunks / 3 total chunks = **0.67**

---

### üö´ **2. Context Precision WITHOUT Reference**

This is **used when you don‚Äôt have a known answer**. Instead, it measures **semantic similarity between the query and the context**.

#### üîç How it's done:

* Use a **similarity function** (like cosine similarity between embeddings of the query and each chunk).
* Set a threshold (e.g., 0.8) to decide if a chunk is relevant.
* Then count how many retrieved chunks meet or exceed that similarity threshold.

#### üìä Formula:

> Precision = # of retrieved chunks with similarity > threshold / total retrieved chunks

---

### üß† Summary Table:

| Metric Variant    | Requires Reference Answer? | Relevance Check         | Use Case                  |
| ----------------- | -------------------------- | ----------------------- | ------------------------- |
| With Reference    | ‚úÖ Yes                      | Matches/supports answer | Ground truth evaluation   |
| Without Reference | ‚ùå No                       | Similarity to query     | Real-time/self-supervised |

---

Would you like Python code examples for either variant (e.g., using `sentence-transformers`)?
